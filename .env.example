# Rename this file to .env and update the values for your environment.
# 이 파일을 .env 로 이름을 바꾸고, 환경에 맞게 값을 수정하세요.

############################################################
# Required Settings (필수 설정)
############################################################

# InfluxDB authentication token used by both ingestion and query APIs.
# InfluxDB 인증 토큰 (수집/조회 API 모두에서 사용) → 운영용 안전한 값으로 교체하세요.
INFLUX_TOKEN=<your-influxdb-token>

# InfluxDB database name where APILog writes/reads analytics events.
# APILog이 데이터를 저장/조회할 InfluxDB 데이터베이스 이름.
INFLUX_DATABASE=<your-database-name>

# Public base URL of the site you want to snapshot/analyze.
# 스냅샷·분석 대상 실서비스의 기본 URL (예: https://example.com).
TARGET_SITE_BASE_URL=<https://your-site.com>

############################################################
# Optional Settings (선택 설정) — 필요한 경우에만 수정
############################################################

# InfluxDB endpoint (override only when not using docker-compose defaults)
INFLUX_URL=http://influxdb3-core:8181

# CORS allow list (comma separated or * for all origins)
CORS_ALLOW_ORIGIN=*

# LLM (Ollama) Settings (used by AI Insights)
LLM_PROVIDER=ollama
LLM_ENDPOINT=http://ollama:11434
LLM_MODEL=llama3:8b
LLM_TEMPERATURE=0.2
LLM_TIMEOUT_S=60

# AI Report LLM (OpenAI) Settings
AI_REPORT_LLM_PROVIDER=openai_compat
AI_REPORT_LLM_ENDPOINT=https://api.openai.com
AI_REPORT_LLM_MODEL=gpt-4.1
AI_REPORT_LLM_API_KEY=
AI_REPORT_LLM_MAX_TOKENS=4096
AI_REPORT_LLM_TEMPERATURE=0.2
AI_REPORT_LLM_TIMEOUT_S=300

# AI caching / internal API endpoints
AI_INSIGHTS_CACHE_TTL=60
AI_INSIGHTS_EXPLAIN_CACHE_TTL=0
AI_REPORT_FETCH_BASE=http://apilog-api:8000

# Where to persist AI-generated dynamic widget specs (JSON file path)
DYNAMIC_WIDGETS_PATH=/snapshots/dynamic_widgets.json

# Optional settings reference (선택 설정 안내)
# - LLM_*: Adjust only for advanced LLM tuning / LLM 동작을 세밀히 조정할 때만 변경
# - AI_INSIGHTS_* / AI_REPORT_FETCH_BASE: Modify when 캐시 정책이나 내부 API 주소를 바꿔야 할 때만 수정하세요.
