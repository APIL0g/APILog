# Docker Compose for APILog Production Environment
# APILog 운영 배포용 Docker Compose 구성 (Prod)
############################################################

services:
  ############################################################
  # 1) InfluxDB 3 Core
  #    Time-series DB (HTTP API on port 8181, local-only access)
  #    시간 기반 데이터 저장소 (8181 포트, 내부 전용)
  ############################################################
  influxdb3-core:
    image: influxdb:3-core
    container_name: influxdb3-core

    environment:
      # Storage type: local filesystem
      # 스토리지 타입: 로컬 파일 시스템
      INFLUXDB3_OBJECT_STORE: file

      # Data directory (persisted to volume)
      # 데이터 디렉터리 (볼륨에 영속 저장)
      INFLUXDB3_DB_DIR: /var/lib/influxdb3

      # Node ID (required argument)
      # 노드 ID (필수 인자)
      INFLUXDB3_NODE_ID: influx-node0

      # Disable auth for self-contained deployments
      # 단일 배포 구성에서는 인증 비활성화
      INFLUXDB3_START_WITHOUT_AUTH: "true"

    command:
      # Entry command with explicit parameters / 필수 인자 명시
      - influxdb3
      - serve
      - --log-filter
      - info
      - --object-store
      - file
      - --plugin-dir
      - /plugins
      - --node-id
      - influx-node0

    # Expose port 8181 only inside the compose network (no host binding)
    # compose 네트워크 내부에서만 8181 포트를 노출 (호스트 포트 사용 안 함)
    expose:
      - "8181"

    volumes:
      # Database / catalog / parquet storage
      # DB / 카탈로그 / Parquet 데이터 저장
      - influx-data:/var/lib/influxdb3
      # Home metadata for influxdb3 user
      # influxdb3 사용자 홈 메타데이터
      - influx-meta:/home/influxdb3/.influxdb3
      # Plugins directory (rollups, custom scripts)
      # 플러그인/커스텀 스크립트 디렉터리
      - influx-plugins:/plugins

    restart: unless-stopped


  ############################################################
  # 2) apilog-api (FastAPI backend)
  #    Event ingestion & query API
  #    이벤트 수집/조회 API
  ############################################################
  apilog-api:
    container_name: apilog-api
    build: ./back/app

    environment:
      # Backend <-> InfluxDB connection info
      # API와 InfluxDB 연결 정보
      INFLUX_URL: ${INFLUX_URL}
      INFLUX_TOKEN: ${INFLUX_TOKEN}
      INFLUX_DATABASE: ${INFLUX_DATABASE}

      # Web API behavior (CORS & LLM)
      # 웹 API 동작 (CORS, LLM)
      CORS_ALLOW_ORIGIN: ${CORS_ALLOW_ORIGIN}
      LLM_PROVIDER: ${LLM_PROVIDER}
      LLM_ENDPOINT: ${LLM_ENDPOINT}
      LLM_MODEL: ${LLM_MODEL}
      LLM_API_KEY: ${LLM_API_KEY}
      LLM_MAX_TOKENS: ${LLM_MAX_TOKENS}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE}
      LLM_TIMEOUT_S: ${LLM_TIMEOUT_S}
      AI_REPORT_LLM_PROVIDER: ${AI_REPORT_LLM_PROVIDER}
      AI_REPORT_LLM_ENDPOINT: ${AI_REPORT_LLM_ENDPOINT}
      AI_REPORT_LLM_MODEL: ${AI_REPORT_LLM_MODEL}
      AI_REPORT_LLM_API_KEY: ${AI_REPORT_LLM_API_KEY}
      AI_REPORT_LLM_MAX_TOKENS: ${AI_REPORT_LLM_MAX_TOKENS}
      AI_REPORT_LLM_TEMPERATURE: ${AI_REPORT_LLM_TEMPERATURE}
      AI_REPORT_LLM_TIMEOUT_S: ${AI_REPORT_LLM_TIMEOUT_S}

      # AI widgets' knobs (cache/internal API/snapshot target)
      # AI 위젯 설정 (캐시/내부 API/스냅샷 대상)
      AI_INSIGHTS_CACHE_TTL: ${AI_INSIGHTS_CACHE_TTL}
      AI_INSIGHTS_EXPLAIN_CACHE_TTL: ${AI_INSIGHTS_EXPLAIN_CACHE_TTL}
      AI_REPORT_FETCH_BASE: ${AI_REPORT_FETCH_BASE}
      TARGET_SITE_BASE_URL: ${TARGET_SITE_BASE_URL}

    depends_on:
      - influxdb3-core
      - ollama

    # Expose backend port internally (reverse proxy consumes it)
    # 내부에서만 8000 포트를 노출 (Nginx가 프록시)
    expose:
      - "8000"

    volumes:
      # Persisted snapshot images mounted at /api/snapshots
      # 히트맵 스냅샷 저장용 볼륨
      - snapshots:/snapshots

    restart: unless-stopped


  ############################################################
  # 3) apilog-nginx (Frontend + Reverse Proxy)
  #    Serves static dashboard + proxies API
  #    프런트 대시보드 및 API 리버스 프록시
  ############################################################
  apilog-nginx:
    container_name: apilog-nginx
    build:
      context: .
      dockerfile: infra/nginx/Dockerfile

    # Map host 10000 -> container 80 (adjust if port conflict occurs)
    # 호스트 10000 포트를 컨테이너 80에 매핑 (충돌 시 수정)
    ports:
      - "10000:80"

    depends_on:
      - apilog-api

    restart: unless-stopped


  ############################################################
  # 4) ollama (Local LLM Server)
  #    Provides local LLM endpoint for AI widgets
  #    AI 위젯에 필요한 로컬 LLM 서버
  ############################################################
  ollama:
    image: ollama/ollama:latest
    container_name: ollama

    environment:
      # Network binding for the Ollama service
      # LLM 서비스 바인딩
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_KEEP_ALIVE: 1h
      # If not provided via .env it falls back to llama3:8b
      # .env에 없으면 기본값(laama3:8b) 사용
      LLM_MODEL: ${LLM_MODEL:-llama3:8b}

    # Only expose internally; FastAPI calls it via service name.
    # 내부에서만 노출, FastAPI가 서비스 이름으로 접근
    expose:
      - "11434"

    volumes:
      # Cache downloaded models
      # 다운로드 모델 캐시
      - ollama-data:/root/.ollama

    entrypoint:
      - /bin/sh
      - -c
      - |
        echo "Bootstrapping Ollama model: $LLM_MODEL"
        ollama serve &
        PID=$!
        sleep 10
        if [ -n "$LLM_MODEL" ]; then
          ollama pull "$LLM_MODEL" || echo "Warning: failed to pull $LLM_MODEL"
        fi
        wait $PID

    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 20

    restart: unless-stopped

    # gpus: all


##############################################################
# Named Volumes (Persistent storage)
# 영속 데이터를 위한 볼륨 정의
##############################################################
volumes:
  influx-data:
  influx-meta:
  influx-plugins:
  snapshots:
  ollama-data:
